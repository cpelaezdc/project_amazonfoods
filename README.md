# project_amazonfoods_review

Project to practice Spark with Azure and AWS.

The source files comes from : http://snap.stanford.edu/data/web-FineFoods.html

###  Original files splited in two differnts  files:

-  FoodsAWS.txt
-  FoodsAzure.txt

The format of source file is not csv nor json:   

```
product/productId: B003OB0IB8
review/userId: AISIYS3D2EAOX
review/profileName: RTyst
review/helpfulness: 1/1
review/score: 5.0
review/time: 1330473600
review/summary: Amazing to the last bite.
review/text: Always being a fan of ramen as a quick and easy meal, finding it on amazon for a decent price and having it delivered to your door by the case is an amazing situation for anyone to find themselves in.

```

They has been converted to json with python scriopt convert_textfile_json.py
[convert_textfile_json.py](convert_textfile_json.py)



```json
[
    {
        "product/productId": "B001E4KFG0",
        "review/userId": "A3SGXH7AUHU8GW",
        "review/profileName": "delmartian",
        "review/helpfulness": "1/1",
        "review/score": "5.0",
        "review/time": "1303862400",
        "review/summary": "Good Quality Dog Food",
        "review/text": "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most."
    }
]

```

there are two json files 

-  foodsAWS.json       (load in AWS S3)
-  foodsAzure.json     (load in Azure Storage Account (container))



## AMAZON AWS
### Create  IAM user
Create a user with autogenerated password
![Alt Text](/images/iamUserCreate.png)

Select AmazonS3FullAccess

![Alt Text](/images/imaUserCreateFullAccess.png)


Download csv file

![Alt Text](/images/imaUserCreateDownloadcsv.png)

Go to Users, Security Credentials and select Create Access Key.

Select Application running outside AWS

![Alt Text](/images/imaUserCreateRunningOutside.png)


Download access Key

![Alt Text](/images/imaUserAccessKey.png)


### Create a Bucket S3

![Alt Text](/images/CreateBucket1.png)

![Alt Text](/images/CreateBucket2.png)

Other settings by default.

Upload file:

![Alt Text](/images/FileinBucket.png)

Make public using ACL

![Alt Text](/images/FileinBucket2.png)



## AZURE PORTAL

### Create an Storage account

Only the settings signaled.  Review and create  

![Alt Text](/images/createStorageAccount.png)
![Alt Text](/images/createStorageAccount.png)

After deployment Go to resource

Create a container:
![Alt Text](/images/createContainer.png)


Optional open Azure Storage Explorer and load json file:
![Alt Text](/images/loadwithAzureStorageExplorer.png)


### Create account Azure DataBricks

![Alt Text](/images/CreateCompteAzureDataBricks.png)

![Alt Text](/images/CreateCompteAzureDataBricks2.png)

![Alt Text](/images/DataBricksGotoResource.png)

![Alt Text](/images/DataBricksLaunch.png)

### Create a cluster

![alt text](/images/DatabricksCompute.png)

Got to Catalog in DataBricks

Go to Default table


![Alt text](/images/CreateTable.png)

Add user_amazonfoods_accessKeys.csv generated in amazon:

![Alt text](/images/addKeyAccessCsv.png)
![Alt text](/images/TableKeyAccess.png)


#### To access to Azure container create a python to autorize access.  


Create a Notebook name s3_amazonfoods.py

![Alt text](/images/CreateNotebook.png)

-  Authorisation notebook with corresponding settings to access data in Azure and AWS.

    
    ![Alt text](/images/AuthorisationPython.png)
    


-  Another notebook to load data and manipulate dataframes (Results inside)

    -  [loadAndTransform.py.ipynb](loadAndTransform.py.ipynb)



